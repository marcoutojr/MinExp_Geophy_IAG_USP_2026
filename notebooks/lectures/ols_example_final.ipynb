{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b44d903c-1bfc-4c13-93d7-0faecdeff9c5",
   "metadata": {},
   "source": [
    "# Ordinary Linear Regression Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5243aff0-df17-4f44-9ac1-0385fa8aad03",
   "metadata": {},
   "source": [
    "# Linear regression as a geophysical inverse problem (matrix form)\n",
    "\n",
    "We model density as a linear function of iron concentration:\n",
    "\n",
    "$$\n",
    "\\rho = a\\,Fe + b,\n",
    "$$\n",
    "\n",
    "with unknown model parameters\n",
    "\n",
    "$$\n",
    "\\mathbf{m} =\n",
    "\\begin{bmatrix}\n",
    "a\\\\\n",
    "b\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Given \\(N\\) measurements \\(\\{(Fe_i,\\rho_i)\\}_{i=1}^N\\), define the data vector\n",
    "\n",
    "$$\n",
    "\\mathbf{d} =\n",
    "\\begin{bmatrix}\n",
    "\\rho_1\\\\\n",
    "\\rho_2\\\\\n",
    "\\vdots\\\\\n",
    "\\rho_N\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{N},\n",
    "$$\n",
    "\n",
    "and the design (kernel) matrix\n",
    "\n",
    "$$\n",
    "\\mathbf{G} =\n",
    "\\begin{bmatrix}\n",
    "Fe_1 & 1\\\\\n",
    "Fe_2 & 1\\\\\n",
    "\\vdots & \\vdots\\\\\n",
    "Fe_N & 1\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{N\\times 2}.\n",
    "$$\n",
    "\n",
    "The linear forward model is:\n",
    "\n",
    "$$\n",
    "\\mathbf{d} \\approx \\mathbf{G}\\,\\mathbf{m}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Data errors, weighting, and geophysical data misfit\n",
    "\n",
    "Assume additive Gaussian noise:\n",
    "\n",
    "$$\n",
    "\\mathbf{d} = \\mathbf{G}\\mathbf{m} + \\boldsymbol{\\epsilon},\n",
    "\\qquad\n",
    "\\boldsymbol{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{C}_d).\n",
    "$$\n",
    "\n",
    "Define the data-weighting matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{W}_d^\\top\\mathbf{W}_d = \\mathbf{C}_d^{-1}.\n",
    "$$\n",
    "\n",
    "For independent errors with standard deviation $\\sigma_i$:\n",
    "\n",
    "$$\n",
    "\\mathbf{W}_d = \\mathrm{diag}\\left(\\frac{1}{\\sigma_1},\\ldots,\\frac{1}{\\sigma_N}\\right).\n",
    "$$\n",
    "\n",
    "Define the residual:\n",
    "\n",
    "$$\n",
    "\\mathbf{r}(\\mathbf{m}) = \\mathbf{G}\\mathbf{m} - \\mathbf{d}.\n",
    "$$\n",
    "\n",
    "We use the data misfit:\n",
    "\n",
    "$$\n",
    "\\phi_d(\\mathbf{m}) = \\|\\mathbf{W}_d\\,\\mathbf{r}(\\mathbf{m})\\|_2^2.\n",
    "$$\n",
    "\n",
    "A common diagnostic is the RMS misfit:\n",
    "\n",
    "$$\n",
    "\\mathrm{RMS}(\\mathbf{m}) = \\sqrt{\\frac{1}{N}\\,\\phi_d(\\mathbf{m})}.\n",
    "$$\n",
    "\n",
    "If “ $\\mathbf{W}_d = \\mathbf{C}_d^{-1/2}$ ” matches the true data uncertainties, then at an optimal fit we expect:\n",
    "\n",
    "$$\n",
    "\\mathrm{RMS}(\\hat{\\mathbf{m}}) \\approx 1\n",
    "\\quad\\text{and}\\quad\n",
    "\\phi_d(\\hat{\\mathbf{m}}) \\approx N.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Three “solutions” we will compare in this notebook\n",
    "\n",
    "### 1) True model (synthetic reference)\n",
    "\n",
    "We generate synthetic data from a known model\n",
    "\n",
    "$$\n",
    "\\mathbf{m}_{true} =\n",
    "\\begin{bmatrix}\n",
    "a_{true}\\\\\n",
    "b_{true}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "and then add noise to produce $\\mathbf{d}$. This is **only available in synthetic experiments**.\n",
    "\n",
    "### 2) Normal-equations solution (closed-form least squares)\n",
    "\n",
    "Minimizing $\\phi_d$ yields the weighted normal equations:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{m}}_{NE}\n",
    "=\n",
    "\\left(\\mathbf{G}^\\top\\mathbf{W}_d^\\top\\mathbf{W}_d\\mathbf{G}\\right)^{-1}\n",
    "\\mathbf{G}^\\top\\mathbf{W}_d^\\top\\mathbf{W}_d\\,\\mathbf{d}\n",
    "$$\n",
    "\n",
    "We solve this $2\\times 2$ system numerically to obtain $\\hat{\\mathbf{m}}_{NE}$.\n",
    "\n",
    "### 3) Optimization (iterative) fit\n",
    "\n",
    "We also estimate the model by iterative optimization (Gradient Descent or Newton):\n",
    "\n",
    "- **Gradient Descent**  \n",
    "  $$\\mathbf{m}_{k+1} = \\mathbf{m}_k - s\\,\\nabla J(\\mathbf{m}_k), \\quad J(\\mathbf{m})=\\tfrac12\\phi_d(\\mathbf{m}).$$\n",
    "\n",
    "- **Newton (damped)**  \n",
    "  $$\\left(\\mathbf{H}+\\lambda\\mathbf{I}\\right)\\,\\Delta\\mathbf{m}_k = \\nabla J(\\mathbf{m}_k),\n",
    "  \\quad \\mathbf{m}_{k+1}=\\mathbf{m}_k-\\alpha_N\\Delta\\mathbf{m}_k.$$\n",
    "\n",
    "At convergence (for this linear least-squares problem), the optimization estimate $\\hat{\\mathbf{m}}_{opt}$ should match the normal-equations solution:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{m}}_{opt} \\to \\hat{\\mathbf{m}}_{NE}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b43b12c-3f5b-4327-9528-cfc584c2a166",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01d51d1f-c752-41d9-9e81-325145f9fa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f344908-e6c7-409e-bd65-46a3d89b6038",
   "metadata": {},
   "source": [
    "## OLS of the density and Fe concentration - GD or Newton Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d59ace-2b68-4daf-9eb3-5f7bb2348398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) Forward model utilities\n",
    "# =============================================================================\n",
    "\n",
    "def build_design_matrix(fe: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build the design matrix G for the linear model rho = a*Fe + b.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fe : (N,) ndarray\n",
    "        Iron concentration values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    G : (N, 2) ndarray\n",
    "        Design matrix with columns [Fe, 1].\n",
    "    \"\"\"\n",
    "    fe = np.asarray(fe).reshape(-1)\n",
    "    return np.column_stack([fe, np.ones_like(fe)])\n",
    "\n",
    "\n",
    "def make_synthetic_data_matrix(\n",
    "    n: int = 60,\n",
    "    a_true: float = 0.6,\n",
    "    b_true: float = 2.5,\n",
    "    fe_min: float = 0.0,\n",
    "    fe_max: float = 10.0,\n",
    "    noise_std: float = 0.4,\n",
    "    seed: int = 0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for the linear inverse problem:\n",
    "\n",
    "        d = G m_true + epsilon,\n",
    "\n",
    "    where epsilon ~ N(0, sigma^2 I) (homoscedastic noise).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Number of samples.\n",
    "    a_true : float\n",
    "        True slope.\n",
    "    b_true : float\n",
    "        True intercept.\n",
    "    fe_min, fe_max : float\n",
    "        Range of Fe values.\n",
    "    noise_std : float\n",
    "        Standard deviation of additive Gaussian noise on rho (same for all points).\n",
    "    seed : int\n",
    "        RNG seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fe : (N,) ndarray\n",
    "        Iron concentration values.\n",
    "    d : (N, 1) ndarray\n",
    "        Noisy densities (data) as a column vector.\n",
    "    d_clean : (N, 1) ndarray\n",
    "        Noise-free densities as a column vector.\n",
    "    G : (N, 2) ndarray\n",
    "        Design matrix.\n",
    "    m_true : (2, 1) ndarray\n",
    "        True model parameters [a_true, b_true]^T.\n",
    "    sigma : (N, 1) ndarray\n",
    "        Per-datum standard deviation (all equal to noise_std here).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    fe = rng.uniform(float(fe_min), float(fe_max), size=int(n))\n",
    "    G = build_design_matrix(fe)\n",
    "\n",
    "    m_true = np.array([[float(a_true)], [float(b_true)]])\n",
    "    d_clean = G @ m_true\n",
    "\n",
    "    eps = rng.normal(loc=0.0, scale=float(noise_std), size=(int(n), 1))\n",
    "    d = d_clean + eps\n",
    "\n",
    "    sigma = np.full((int(n), 1), float(noise_std))\n",
    "    return fe, d, d_clean, G, m_true, sigma\n",
    "\n",
    "\n",
    "def predict_data(G: np.ndarray, m: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Forward prediction:\n",
    "\n",
    "        d_pred = G m\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : (N, 2) ndarray\n",
    "        Design matrix.\n",
    "    m : (2, 1) ndarray\n",
    "        Model parameters [a, b]^T.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    d_pred : (N, 1) ndarray\n",
    "        Predicted data.\n",
    "    \"\"\"\n",
    "    return G @ m\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) Data weighting and misfit\n",
    "# =============================================================================\n",
    "\n",
    "def build_Wd_from_sigma(sigma: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build a diagonal data-weighting matrix Wd from per-datum standard deviations.\n",
    "\n",
    "    For independent errors:\n",
    "        C_d = diag(sigma_i^2)\n",
    "        Wd = C_d^{-1/2} = diag(1/sigma_i)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : (N, 1) or (N,) ndarray\n",
    "        Standard deviations for each datum.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Wd : (N, N) ndarray\n",
    "        Diagonal weighting matrix.\n",
    "    \"\"\"\n",
    "    sigma = np.asarray(sigma).reshape(-1)\n",
    "    if np.any(sigma <= 0):\n",
    "        raise ValueError(\"All sigma values must be > 0.\")\n",
    "    return np.diag(1.0 / sigma)\n",
    "\n",
    "\n",
    "def residual_vector(G: np.ndarray, d: np.ndarray, m: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the residual vector:\n",
    "\n",
    "        r(m) = G m - d\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : (N, 2) ndarray\n",
    "        Design matrix.\n",
    "    d : (N, 1) ndarray\n",
    "        Observed data.\n",
    "    m : (2, 1) ndarray\n",
    "        Model parameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    r : (N, 1) ndarray\n",
    "        Residual vector.\n",
    "    \"\"\"\n",
    "    return (G @ m) - d\n",
    "\n",
    "\n",
    "def weighted_residual(G: np.ndarray, d: np.ndarray, m: np.ndarray, Wd: np.ndarray | None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the (optionally) weighted residual:\n",
    "\n",
    "        r_w(m) = Wd * (G m - d)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : (N, 2) ndarray\n",
    "        Design matrix.\n",
    "    d : (N, 1) ndarray\n",
    "        Observed data.\n",
    "    m : (2, 1) ndarray\n",
    "        Model parameters.\n",
    "    Wd : (N, N) ndarray or None\n",
    "        Data weighting matrix. If None, returns the unweighted residual.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    r_w : (N, 1) ndarray\n",
    "        Weighted residual vector (or unweighted if Wd is None).\n",
    "    \"\"\"\n",
    "    r = residual_vector(G, d, m)\n",
    "    return r if Wd is None else (Wd @ r)\n",
    "\n",
    "\n",
    "def data_misfit_phi_d(G: np.ndarray, d: np.ndarray, m: np.ndarray, Wd: np.ndarray | None) -> float:\n",
    "    \"\"\"\n",
    "    Data misfit in geophysical inversion form (NOT normalized by N):\n",
    "\n",
    "        phi_d(m) = || Wd (G m - d) ||_2^2\n",
    "\n",
    "    If Wd is None, this reduces to:\n",
    "        phi_d(m) = || G m - d ||_2^2\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : (N, 2) ndarray\n",
    "        Design matrix.\n",
    "    d : (N, 1) ndarray\n",
    "        Observed data.\n",
    "    m : (2, 1) ndarray\n",
    "        Model parameters.\n",
    "    Wd : (N, N) ndarray or None\n",
    "        Data weighting matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    phi_d : float\n",
    "        Scalar data misfit.\n",
    "    \"\"\"\n",
    "    rw = weighted_residual(G, d, m, Wd)\n",
    "    return float((rw.T @ rw)[0, 0])\n",
    "\n",
    "\n",
    "def rms_misfit(G: np.ndarray, d: np.ndarray, m: np.ndarray, Wd: np.ndarray | None) -> float:\n",
    "    \"\"\"\n",
    "    RMS misfit associated with the data misfit:\n",
    "\n",
    "        RMS(m) = sqrt( phi_d(m) / N )\n",
    "\n",
    "    Interpretation:\n",
    "    - If Wd = C_d^{-1/2} and the noise model is correct, RMS ~ 1 at optimal fit.\n",
    "    Newton damping\n",
    "    --------------\n",
    "    In the Newton update we sometimes solve (H + λ I) Δm = ∇φ, where λ ≥ 0 is a *damping*\n",
    "    (Levenberg-style) parameter that stabilizes the solve if H is ill-conditioned.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : (N, 2) ndarray\n",
    "        Design matrix.\n",
    "    d : (N, 1) ndarray\n",
    "        Observed data.\n",
    "    m : (2, 1) ndarray\n",
    "        Model parameters.\n",
    "    Wd : (N, N) ndarray or None\n",
    "        Data weighting matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rms : float\n",
    "        RMS misfit.\n",
    "    \"\"\"\n",
    "    N = G.shape[0]\n",
    "    return float(np.sqrt(data_misfit_phi_d(G, d, m, Wd) / N))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) Gradient, Hessian, and solvers (GD + Newton + Normal Equations)\n",
    "# =============================================================================\n",
    "\n",
    "def grad_phi_d(G: np.ndarray, d: np.ndarray, m: np.ndarray, Wd: np.ndarray | None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Gradient of the geophysical data misfit:\n",
    "\n",
    "        phi_d(m) = || Wd (G m - d) ||_2^2\n",
    "\n",
    "    Let r = (G m - d). Then:\n",
    "        phi_d = r^T (Wd^T Wd) r\n",
    "        ∇phi_d = 2 G^T (Wd^T Wd) r\n",
    "\n",
    "    If Wd is None, Wd^T Wd = I.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : (N, 2) ndarray\n",
    "        Design matrix.\n",
    "    d : (N, 1) ndarray\n",
    "        Observed data.\n",
    "    m : (2, 1) ndarray\n",
    "        Model parameters.\n",
    "    Wd : (N, N) ndarray or None\n",
    "        Data weighting matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grad : (2, 1) ndarray\n",
    "        Gradient vector.\n",
    "    \"\"\"\n",
    "    r = residual_vector(G, d, m)  # (N,1)\n",
    "    if Wd is None:\n",
    "        A = r\n",
    "    else:\n",
    "        A = (Wd.T @ Wd) @ r\n",
    "    return 2.0 * (G.T @ A)\n",
    "\n",
    "\n",
    "def hessian_phi_d(G: np.ndarray, Wd: np.ndarray | None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Hessian of the geophysical data misfit:\n",
    "\n",
    "        phi_d(m) = || Wd (G m - d) ||_2^2\n",
    "\n",
    "    For linear G, the Hessian is constant:\n",
    "        H = 2 G^T (Wd^T Wd) G\n",
    "\n",
    "    If Wd is None, Wd^T Wd = I.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : (N, 2) ndarray\n",
    "        Design matrix.\n",
    "    Wd : (N, N) ndarray or None\n",
    "        Data weighting matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    H : (2, 2) ndarray\n",
    "        Hessian matrix.\n",
    "    \"\"\"\n",
    "    if Wd is None:\n",
    "        return 2.0 * (G.T @ G)\n",
    "    return 2.0 * (G.T @ (Wd.T @ Wd) @ G)\n",
    "\n",
    "\n",
    "def gd_step(G: np.ndarray, d: np.ndarray, m: np.ndarray, Wd: np.ndarray | None, alpha: float):\n",
    "    \"\"\"\n",
    "    One *stable* gradient descent step for the geophysical data misfit.\n",
    "\n",
    "    We keep the geophysical (non-normalized) misfit definition:\n",
    "\n",
    "        phi_d(m) = || Wd (G m - d) ||_2^2\n",
    "\n",
    "    but we perform gradient descent on the equivalent quadratic objective\n",
    "\n",
    "        J(m) = (1/2) * phi_d(m),\n",
    "\n",
    "    which has the same minimizer and a cleaner gradient/Hessian.\n",
    "\n",
    "    A common reason for \"diverging\" behavior after switching from a machine-learning\n",
    "    style loss (e.g., (1/2N)||r||^2) to a geophysical misfit (||Wd r||^2) is that the\n",
    "    gradient magnitude scales with:\n",
    "      - the number of data N, and\n",
    "      - the weighting level (roughly sigma^{-2} when Wd = C_d^{-1/2}).\n",
    "\n",
    "    To make the iteration robust without hand-tuning tiny learning rates, we use a\n",
    "    Lipschitz-stable step size based on the largest eigenvalue of the Hessian:\n",
    "\n",
    "        grad J(m) = G^T Wd^T Wd (G m - d)\n",
    "        H_J       = G^T Wd^T Wd G\n",
    "        s         = alpha / lambda_max(H_J)\n",
    "        m_{k+1}   = m_k - s * grad J(m_k)\n",
    "\n",
    "    Here, ``alpha`` is a dimensionless step fraction (typically 0 < alpha <= 1).\n",
    "    For a strictly convex quadratic, choosing alpha <= 1 ensures monotone decrease\n",
    "    for well-behaved problems; alpha close to 1 is usually fastest.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : (N, 2) ndarray\n",
    "        Design matrix.\n",
    "    d : (N, 1) ndarray\n",
    "        Observed data.\n",
    "    m : (2, 1) ndarray\n",
    "        Current model.\n",
    "    Wd : (N, N) ndarray or None\n",
    "        Data weighting matrix (Wd = C_d^{-1/2}). If None, Wd is treated as identity.\n",
    "    alpha : float\n",
    "        Dimensionless step fraction in (0, 1]. This is *not* an absolute learning rate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    m_new : (2, 1) ndarray\n",
    "        Updated model.\n",
    "    grad_norm : float\n",
    "        Euclidean norm of grad J at the current iterate (not of grad phi_d).\n",
    "    step_size : float\n",
    "        Actual step size ``s = alpha / lambda_max(H_J)`` used in this update.\n",
    "    lambda_max : float\n",
    "        Largest eigenvalue of H_J (a Lipschitz constant for grad J).\n",
    "    \"\"\"\n",
    "    alpha = float(alpha)\n",
    "    if not (alpha > 0.0):\n",
    "        raise ValueError(\"alpha must be > 0\")\n",
    "    # grad_phi_d = 2 * G^T WTW r  -> grad_J = 0.5 * grad_phi_d\n",
    "    gJ = 0.5 * grad_phi_d(G, d, m, Wd)\n",
    "\n",
    "    # hessian_phi_d = 2 * G^T WTW G -> H_J = 0.5 * hessian_phi_d\n",
    "    HJ = 0.5 * hessian_phi_d(G, Wd)\n",
    "\n",
    "    # Largest eigenvalue of a 2x2 SPD matrix (use eigvalsh for numerical stability)\n",
    "    evals = np.linalg.eigvalsh(HJ)\n",
    "    lambda_max = float(np.max(evals))\n",
    "\n",
    "    # Avoid divide-by-zero in pathological cases (e.g., degenerate design matrix)\n",
    "    eps = 1e-30\n",
    "    step_size = alpha / (lambda_max + eps)\n",
    "\n",
    "    m_new = m - step_size * gJ\n",
    "    return m_new, float(np.linalg.norm(gJ)), float(step_size), float(lambda_max)\n",
    "\n",
    "\n",
    "def newton_step(G: np.ndarray, d: np.ndarray, m: np.ndarray, Wd: np.ndarray | None, damping: float = 0.0, step_scale: float = 1.0):\n",
    "    \"\"\"\n",
    "    One (optionally damped) Newton step on the geophysical misfit:\n",
    "\n",
    "        phi_d(m) = || Wd (G m - d) ||^2\n",
    "\n",
    "    Newton solves:\n",
    "        (H + damping*I) delta = grad\n",
    "        m_new = m - step_scale * delta\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - For this 2-parameter linear problem, Newton typically reaches the minimizer\n",
    "      in one step (up to numerical precision), especially with step_scale=1 and small damping.\n",
    "    - damping >= 0 provides Levenberg-style stabilization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : (N, 2) ndarray\n",
    "        Design matrix.\n",
    "    d : (N, 1) ndarray\n",
    "        Observed data.\n",
    "    m : (2, 1) ndarray\n",
    "        Current model.\n",
    "    Wd : (N, N) ndarray or None\n",
    "        Data weighting matrix.\n",
    "    damping : float\n",
    "        Damping coefficient λ >= 0.\n",
    "    step_scale : float\n",
    "        Step scale α_N in (0, 1]; 1 is standard Newton.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    m_new : (2, 1) ndarray\n",
    "        Updated model.\n",
    "    grad_norm : float\n",
    "        Norm of gradient at current iterate.\n",
    "    step_norm : float\n",
    "        Norm of Newton step delta.\n",
    "    \"\"\"\n",
    "    damping = float(damping)\n",
    "    if damping < 0:\n",
    "        raise ValueError(\"damping must be >= 0\")\n",
    "\n",
    "    step_scale = float(step_scale)\n",
    "\n",
    "    g = grad_phi_d(G, d, m, Wd)\n",
    "    H = hessian_phi_d(G, Wd)\n",
    "    H_d = H + damping * np.eye(H.shape[0])\n",
    "\n",
    "    try:\n",
    "        delta = np.linalg.solve(H_d, g)\n",
    "    except np.linalg.LinAlgError:\n",
    "        delta, *_ = np.linalg.lstsq(H_d, g, rcond=None)\n",
    "\n",
    "    m_new = m - step_scale * delta\n",
    "    return m_new, float(np.linalg.norm(g)), float(np.linalg.norm(delta))\n",
    "\n",
    "\n",
    "def normal_equation_solution(G: np.ndarray, d: np.ndarray, Wd: np.ndarray | None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the normal-equation (closed-form) solution for weighted least squares:\n",
    "\n",
    "        minimize ||Wd (G m - d)||^2\n",
    "\n",
    "    Normal equations:\n",
    "        (G^T Wd^T Wd G) m = (G^T Wd^T Wd d)\n",
    "\n",
    "    If Wd is None, this reduces to:\n",
    "        (G^T G) m = (G^T d)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : (N, 2) ndarray\n",
    "        Design matrix.\n",
    "    d : (N, 1) ndarray\n",
    "        Observed data.\n",
    "    Wd : (N, N) ndarray or None\n",
    "        Data weighting matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    m_ne : (2, 1) ndarray\n",
    "        Normal-equation solution.\n",
    "    \"\"\"\n",
    "    if Wd is None:\n",
    "        A = G.T @ G\n",
    "        b = G.T @ d\n",
    "    else:\n",
    "        WTW = Wd.T @ Wd\n",
    "        A = G.T @ WTW @ G\n",
    "        b = G.T @ WTW @ d\n",
    "    return np.linalg.solve(A, b)\n",
    "\n",
    "\n",
    "def misfit_surface_grid(G: np.ndarray, d: np.ndarray, Wd: np.ndarray | None, a_vals: np.ndarray, b_vals: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute phi_d(a,b) on a grid (vectorized), where:\n",
    "\n",
    "        phi_d(m) = || Wd (G m - d) ||^2\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : (N, 2) ndarray\n",
    "        Design matrix (first col is Fe, second is ones).\n",
    "    d : (N, 1) ndarray\n",
    "        Observed data.\n",
    "    Wd : (N, N) ndarray or None\n",
    "        Data weighting matrix.\n",
    "    a_vals : (Na,) ndarray\n",
    "        Grid values for slope a.\n",
    "    b_vals : (Nb,) ndarray\n",
    "        Grid values for intercept b.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Phi : (Nb, Na) ndarray\n",
    "        Misfit surface values.\n",
    "    \"\"\"\n",
    "    N = G.shape[0]\n",
    "    Fe = G[:, 0:1]   # (N,1)\n",
    "    ones = G[:, 1:2] # (N,1)\n",
    "\n",
    "    A, B = np.meshgrid(a_vals, b_vals)  # (Nb,Na)\n",
    "    A_flat = A.reshape(1, -1)\n",
    "    B_flat = B.reshape(1, -1)\n",
    "\n",
    "    d_pred = Fe @ A_flat + ones @ B_flat  # (N, Nb*Na)\n",
    "    r = d_pred - d                         # (N, Nb*Na)\n",
    "\n",
    "    if Wd is not None:\n",
    "        r = Wd @ r  # still works: (N,N)@(N,M) -> (N,M)\n",
    "\n",
    "    phi_flat = np.sum(r**2, axis=0)        # ||r||^2 (no /N)\n",
    "    return phi_flat.reshape(A.shape)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4) State container\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class RegressionState:\n",
    "    \"\"\"\n",
    "    Container for dataset + inversion state.\n",
    "\n",
    "    Stores:\n",
    "    - data (fe, G, d, d_clean, sigma),\n",
    "    - weighting matrix Wd (or None),\n",
    "    - true model m_true (for synthetic reference),\n",
    "    - normal-equation solution m_ne,\n",
    "    - current model estimate m,\n",
    "    - histories for phi_d, RMS, gradient norms, Newton step norms,\n",
    "    - path in (a,b) space.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    fe : (N,) ndarray\n",
    "        Iron concentration samples (for plotting).\n",
    "    G : (N,2) ndarray\n",
    "        Design matrix.\n",
    "    d : (N,1) ndarray\n",
    "        Observed data.\n",
    "    d_clean : (N,1) ndarray\n",
    "        Noise-free data (synthetic reference).\n",
    "    sigma : (N,1) ndarray\n",
    "        Per-datum standard deviations used to generate synthetic noise.\n",
    "    Wd : (N,N) ndarray or None\n",
    "        Data weighting matrix. If None, inversion is unweighted.\n",
    "    m_true : (2,1) ndarray\n",
    "        True model parameters used for simulation.\n",
    "    m_ne : (2,1) ndarray\n",
    "        Normal-equation solution (weighted or unweighted, consistent with Wd).\n",
    "    m : (2,1) ndarray\n",
    "        Current estimate.\n",
    "    it : int\n",
    "        Iteration counter.\n",
    "    phi_history : list[float]\n",
    "        Data misfit values phi_d(m) per iteration (includes iteration 0).\n",
    "    rms_history : list[float]\n",
    "        RMS misfit values per iteration (includes iteration 0).\n",
    "    gradnorm_history : list[float]\n",
    "        Norm of gradient per iteration step (after each step).\n",
    "    stepnorm_history : list[float]\n",
    "        Newton step norm per iteration step (NaN for GD).\n",
    "    m_history : list[np.ndarray]\n",
    "        Model vector at each iteration (includes iteration 0).\n",
    "    \"\"\"\n",
    "    fe: np.ndarray = field(default_factory=lambda: np.array([]))\n",
    "    G: np.ndarray = field(default_factory=lambda: np.empty((0, 2)))\n",
    "    d: np.ndarray = field(default_factory=lambda: np.empty((0, 1)))\n",
    "    d_clean: np.ndarray = field(default_factory=lambda: np.empty((0, 1)))\n",
    "    sigma: np.ndarray = field(default_factory=lambda: np.empty((0, 1)))\n",
    "\n",
    "    Wd: np.ndarray | None = None\n",
    "\n",
    "    m_true: np.ndarray = field(default_factory=lambda: np.zeros((2, 1)))\n",
    "    m_ne: np.ndarray = field(default_factory=lambda: np.zeros((2, 1)))\n",
    "    m: np.ndarray = field(default_factory=lambda: np.zeros((2, 1)))\n",
    "\n",
    "    it: int = 0\n",
    "    phi_history: list = field(default_factory=list)\n",
    "    rms_history: list = field(default_factory=list)\n",
    "    gradnorm_history: list = field(default_factory=list)\n",
    "    stepnorm_history: list = field(default_factory=list)\n",
    "    m_history: list = field(default_factory=list)\n",
    "\n",
    "    def reset_model(self, a0: float = 0.0, b0: float = 0.0):\n",
    "        \"\"\"\n",
    "        Reset current model estimate and all histories.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        a0, b0 : float\n",
    "            Initial guess for slope and intercept.\n",
    "        \"\"\"\n",
    "        self.m = np.array([[float(a0)], [float(b0)]])\n",
    "        self.it = 0\n",
    "\n",
    "        phi0 = data_misfit_phi_d(self.G, self.d, self.m, self.Wd)\n",
    "        rms0 = rms_misfit(self.G, self.d, self.m, self.Wd)\n",
    "\n",
    "        self.phi_history = [phi0]\n",
    "        self.rms_history = [rms0]\n",
    "        self.gradnorm_history = []\n",
    "        self.stepnorm_history = []\n",
    "        self.m_history = [self.m.copy()]\n",
    "\n",
    "    @property\n",
    "    def a(self) -> float:\n",
    "        \"\"\"Current slope estimate.\"\"\"\n",
    "        return float(self.m[0, 0])\n",
    "\n",
    "    @property\n",
    "    def b(self) -> float:\n",
    "        \"\"\"Current intercept estimate.\"\"\"\n",
    "        return float(self.m[1, 0])\n",
    "\n",
    "    def update_normal_equations_solution(self):\n",
    "        \"\"\"\n",
    "        Recompute the normal-equation solution m_ne consistent with current Wd.\n",
    "        \"\"\"\n",
    "        self.m_ne = normal_equation_solution(self.G, self.d, self.Wd)\n",
    "\n",
    "    def step(self, method: str, lr: float = 1e-3, damping: float = 0.0, step_scale: float = 1.0):\n",
    "        \"\"\"\n",
    "        Perform one iteration using either gradient descent or Newton.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        method : str\n",
    "            \"gd\" or \"newton\".\n",
    "        lr : float\n",
    "            Learning rate for GD. Ignored for Newton.\n",
    "        damping : float\n",
    "            Newton damping λ >= 0. Ignored for GD.\n",
    "        step_scale : float\n",
    "            Newton step scale α_N in (0,1]. Ignored for GD.\n",
    "        \"\"\"\n",
    "        method = method.lower().strip()\n",
    "\n",
    "        \n",
    "        if method == \"gd\":\n",
    "            # Here lr is interpreted as a *dimensionless* alpha in (0, 1], not an absolute learning rate.\n",
    "            self.m, gnorm, step_size, lambda_max = gd_step(self.G, self.d, self.m, self.Wd, alpha=float(lr))\n",
    "            # Store the actual step length in stepnorm_history for GD (useful diagnostics).\n",
    "            snorm = float(step_size)\n",
    "        \n",
    "        elif method == \"newton\":\n",
    "            self.m, gnorm, snorm = newton_step(self.G, self.d, self.m, self.Wd, damping=float(damping), step_scale=float(step_scale))\n",
    "        else:\n",
    "            raise ValueError(\"method must be 'gd' or 'newton'\")\n",
    "        \n",
    "        self.it += 1\n",
    "    \n",
    "        phi = data_misfit_phi_d(self.G, self.d, self.m, self.Wd)\n",
    "        rms = rms_misfit(self.G, self.d, self.m, self.Wd)\n",
    "    \n",
    "        self.phi_history.append(phi)\n",
    "        self.rms_history.append(rms)\n",
    "        self.gradnorm_history.append(float(gnorm))\n",
    "        self.stepnorm_history.append(float(snorm))\n",
    "        self.m_history.append(self.m.copy())\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5) Plotting + summary\n",
    "# =============================================================================\n",
    "\n",
    "def plot_fits_and_histories(state: RegressionState, show_true: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Plot:\n",
    "      1) data + current fit + normal-equation fit (+ true line if requested),\n",
    "      2) data misfit history phi_d with phi_d(m_NE) reference,\n",
    "      3) RMS misfit history with RMS(m_NE) and target RMS ≈ 1.\n",
    "\n",
    "    This version avoids Matplotlib mathtext failures in legends/tight_layout by:\n",
    "      - removing outer '$...$' delimiters in labels\n",
    "      - using constrained_layout instead of tight_layout\n",
    "      - closing the figure explicitly after show()\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : RegressionState\n",
    "        Current inversion state.\n",
    "    show_true : bool\n",
    "        If True, overlay the synthetic \"true\" line for reference.\n",
    "    \"\"\"\n",
    "    fe = state.fe\n",
    "    if fe.size == 0:\n",
    "        return\n",
    "\n",
    "    d = state.d.reshape(-1)\n",
    "\n",
    "    x_line = np.linspace(fe.min(), fe.max(), 250)\n",
    "    G_line = build_design_matrix(x_line)\n",
    "\n",
    "    y_cur = (G_line @ state.m).reshape(-1)\n",
    "    y_ne = (G_line @ state.m_ne).reshape(-1)\n",
    "    y_true = (G_line @ state.m_true).reshape(-1)\n",
    "\n",
    "    phi_ne = data_misfit_phi_d(state.G, state.d, state.m_ne, state.Wd)\n",
    "    rms_ne = rms_misfit(state.G, state.d, state.m_ne, state.Wd)\n",
    "\n",
    "    fig = plt.figure(figsize=(14, 4), constrained_layout=True)\n",
    "\n",
    "    ax1 = fig.add_subplot(1, 3, 1)\n",
    "    ax1.scatter(fe, d, marker=\"o\", color=\"k\")\n",
    "    ax1.plot(x_line, y_cur, linewidth=2, color=\"limegreen\", label=\"fit (current)\")\n",
    "    ax1.plot(x_line, y_ne, linewidth=2, linestyle=\"--\", color=\"tab:blue\", label=\"fit (Normal Eq.)\")\n",
    "    if show_true:\n",
    "        ax1.plot(x_line, y_true, linewidth=2, linestyle=\"--\", color=\"r\", label=\"true\")\n",
    "    ax1.set_xlabel(\"Fe concentration (%)\")\n",
    "    ax1.set_ylabel(\"Density (g/cm³)\")\n",
    "    ax1.set_title(\"Data and fits\")\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2 = fig.add_subplot(1, 3, 2)\n",
    "    # --- IMPORTANT: no $...$ wrapper, prevents your mathtext crash ---\n",
    "    ax2.plot(state.phi_history, color=\"limegreen\", label=r\"$\\phi_d(m)$\")\n",
    "    ax2.axhline(phi_ne, linestyle=\"--\", color=\"tab:blue\", label=r\"$\\phi_d(m_{NE})$\")\n",
    "    ax2.set_xlabel(\"Iteration\")\n",
    "    ax2.set_ylabel(r\"$\\phi_d$\")\n",
    "    ax2.set_title(\"Data misfit vs iteration\")\n",
    "    ax2.legend()\n",
    "\n",
    "    ax3 = fig.add_subplot(1, 3, 3)\n",
    "    ax3.plot(state.rms_history, color=\"limegreen\", label=\"RMS(m)\")\n",
    "    ax3.axhline(rms_ne, linestyle=\"--\", color=\"tab:blue\", label=\"RMS(m_NE)\")\n",
    "    ax3.axhline(1.0, linestyle=\":\", color=\"r\", label=\"target RMS ≈ 1\")\n",
    "    ax3.set_xlabel(\"Iteration\")\n",
    "    ax3.set_ylabel(\"RMS misfit\")\n",
    "    ax3.set_title(\"RMS misfit vs iteration\")\n",
    "    ax3.legend()\n",
    "\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_misfit_surface(\n",
    "    state: RegressionState,\n",
    "    span_factor: float = 2.0,\n",
    "    ngrid: int = 140,\n",
    "    log_surface: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot a contour (or image fallback) of the data misfit surface phi_d(a,b),\n",
    "    overlaid with:\n",
    "      - solver path in (a,b),\n",
    "      - normal-equation solution,\n",
    "      - true model.\n",
    "\n",
    "    The function closes the figure after display to prevent duplicate/empty figures\n",
    "    in widget output contexts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : RegressionState\n",
    "        Current inversion state.\n",
    "    span_factor : float\n",
    "        Window around (a_NE, b_NE) sized by span_factor*max(|param|,1).\n",
    "    ngrid : int\n",
    "        Grid points per axis.\n",
    "    log_surface : bool\n",
    "        If True, plot log10(phi_d + eps).\n",
    "    \"\"\"\n",
    "    if state.G.size == 0:\n",
    "        return\n",
    "\n",
    "    a_ne = float(state.m_ne[0, 0])\n",
    "    b_ne = float(state.m_ne[1, 0])\n",
    "\n",
    "    a_span = float(span_factor) * max(abs(a_ne), 1.0)\n",
    "    b_span = float(span_factor) * max(abs(b_ne), 1.0)\n",
    "\n",
    "    a_vals = np.linspace(a_ne - a_span, a_ne + a_span, int(ngrid))\n",
    "    b_vals = np.linspace(b_ne - b_span, b_ne + b_span, int(ngrid))\n",
    "\n",
    "    Phi = misfit_surface_grid(state.G, state.d, state.Wd, a_vals, b_vals)\n",
    "\n",
    "    # sanitize\n",
    "    finite = np.isfinite(Phi)\n",
    "    fill = float(np.nanmax(Phi[finite])) if np.any(finite) else 0.0\n",
    "    Phi = np.nan_to_num(Phi, nan=fill, posinf=fill, neginf=0.0)\n",
    "\n",
    "    if log_surface:\n",
    "        eps = 1e-18\n",
    "        Z = np.log10(Phi + eps)\n",
    "        title = r\"$\\log_{10}(\\phi_d(a,b))$ with path\"\n",
    "    else:\n",
    "        Z = Phi\n",
    "        title = r\"$\\phi_d(a,b)$ with path\"\n",
    "\n",
    "    zmin = float(np.nanmin(Z))\n",
    "    zmax = float(np.nanmax(Z))\n",
    "\n",
    "    path = np.hstack(state.m_history) if state.m_history else np.array([[a_ne], [b_ne]])\n",
    "    a_path, b_path = path[0, :], path[1, :]\n",
    "\n",
    "    a_true = float(state.m_true[0, 0])\n",
    "    b_true = float(state.m_true[1, 0])\n",
    "\n",
    "    fig = plt.figure(figsize=(7.2, 6.2))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    if not np.isfinite(zmin) or not np.isfinite(zmax) or abs(zmax - zmin) < 1e-12:\n",
    "        im = ax.imshow(\n",
    "            Z,\n",
    "            origin=\"lower\",\n",
    "            aspect=\"auto\",\n",
    "            extent=[a_vals.min(), a_vals.max(), b_vals.min(), b_vals.max()],\n",
    "        )\n",
    "        ax.set_title(title + \" (imshow fallback)\")\n",
    "        fig.colorbar(im, ax=ax, shrink=0.85)\n",
    "    else:\n",
    "        levels = np.linspace(zmin, zmax, 20)\n",
    "        ax.contour(a_vals, b_vals, Z, levels=levels)\n",
    "        ax.set_title(title)\n",
    "\n",
    "    ax.plot(a_path, b_path, marker=\"o\", markersize=3, linewidth=1, color=\"k\", label=\"path\")\n",
    "    ax.scatter([a_ne], [b_ne], marker=\"x\", s=80, label=\"Normal Eq.\")\n",
    "    ax.scatter([a_true], [b_true], marker=\"*\", s=120, label=\"True\")\n",
    "\n",
    "    ax.set_xlabel(\"a (slope)\")\n",
    "    ax.set_ylabel(\"b (intercept)\")\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def state_summary_text(state: RegressionState, solver_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Build a human-readable summary of the current inversion state.\n",
    "\n",
    "    This summary is intended for interactive notebook use (e.g., ipywidgets callbacks).\n",
    "    It reports the current parameter estimate, reference solutions (normal-equation and\n",
    "    true model), and key diagnostics consistent with geophysical inversion practice.\n",
    "\n",
    "    Definitions\n",
    "    -----------\n",
    "    Data misfit (not normalized by N):\n",
    "        phi_d(m) = ||Wd (G m - d)||_2^2\n",
    "\n",
    "    RMS misfit:\n",
    "        RMS(m) = sqrt( phi_d(m) / N )\n",
    "\n",
    "    Interpretation:\n",
    "    - If Wd = C_d^{-1/2} and the noise model is correct, RMS ~ 1 at optimal fit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : RegressionState\n",
    "        State container holding data, weighting, current model, reference solutions,\n",
    "        and iteration histories.\n",
    "    solver_name : str\n",
    "        Human-readable name of the active solver (e.g., \"Gradient Descent\" or \"Newton\").\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    text : str\n",
    "        Multi-line formatted string for printing.\n",
    "    \"\"\"\n",
    "    phi_cur = state.phi_history[-1] if state.phi_history else np.nan\n",
    "    rms_cur = state.rms_history[-1] if state.rms_history else np.nan\n",
    "\n",
    "    phi_ne = data_misfit_phi_d(state.G, state.d, state.m_ne, state.Wd)\n",
    "    rms_ne = rms_misfit(state.G, state.d, state.m_ne, state.Wd)\n",
    "\n",
    "    gnorm = state.gradnorm_history[-1] if state.gradnorm_history else np.nan\n",
    "    snorm = state.stepnorm_history[-1] if state.stepnorm_history else np.nan\n",
    "\n",
    "    a_true = float(state.m_true[0, 0])\n",
    "    b_true = float(state.m_true[1, 0])\n",
    "\n",
    "    a_ne = float(state.m_ne[0, 0])\n",
    "    b_ne = float(state.m_ne[1, 0])\n",
    "\n",
    "    err_to_ne = float(np.linalg.norm(state.m - state.m_ne))\n",
    "\n",
    "    weighting_label = \"weighted (Wd = C_d^{-1/2})\" if state.Wd is not None else \"unweighted (Wd = I)\"\n",
    "\n",
    "    return (\n",
    "        f\"Solver: {solver_name}\\n\"\n",
    "        f\"Weighting: {weighting_label}\\n\"\n",
    "        f\"Iteration: {state.it}\\n\"\n",
    "        f\"Current        m = [a, b]^T = [{state.a:.6f}, {state.b:.6f}]^T\\n\"\n",
    "        f\"Normal Eq. sol m = [a, b]^T = [{a_ne:.6f}, {b_ne:.6f}]^T\\n\"\n",
    "        f\"True           m = [a, b]^T = [{a_true:.6f}, {b_true:.6f}]^T\\n\"\n",
    "        f\"||m - m_NE||:  {err_to_ne:.6e}\\n\"\n",
    "        f\"phi_d(m):      {phi_cur:.6e}     RMS(m):      {rms_cur:.6e}\\n\"\n",
    "        f\"phi_d(m_NE):   {phi_ne:.6e}     RMS(m_NE):   {rms_ne:.6e}\\n\"\n",
    "        f\"||∇phi_d||:    {gnorm:.6e}\\n\"\n",
    "        f\"Step metric:  {snorm:.6e}  (GD: step size s; Newton: ||Δm||)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4eef26-a574-4132-bffd-e15ffe8960d1",
   "metadata": {},
   "source": [
    "## Interactive widgets (iterations step-by-step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8be5cda-7062-4d0d-9b0b-a4980ce590f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b941d37cf98466bbf20d67abc2e056f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 6) Interactive UI (widgets) - robust, single-instance + per-instance callbacks\n",
    "# =============================================================================\n",
    "\n",
    "import traceback\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Single-instance management\n",
    "# -----------------------------------------------------------------------------\n",
    "# We keep ONE persistent app_out and close prior widget instances on re-run.\n",
    "if \"_FE_RHO_APP\" in globals():\n",
    "    try:\n",
    "        # Reuse the same container output so the UI doesn't duplicate in the notebook\n",
    "        _prev_out = _FE_RHO_APP.get(\"app_out\", None)\n",
    "        if isinstance(_prev_out, widgets.Output):\n",
    "            app_out = _prev_out\n",
    "        # Close prior widgets to stop callbacks and accidental cross-wiring\n",
    "        for w in _FE_RHO_APP.get(\"widgets\", []):\n",
    "            try:\n",
    "                w.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# If no previous container exists, create it once and display it once\n",
    "if \"app_out\" not in globals() or not isinstance(app_out, widgets.Output):\n",
    "    app_out = widgets.Output()\n",
    "    display(app_out)\n",
    "\n",
    "# Close stray matplotlib figures to avoid blank/extra renders\n",
    "plt.close(\"all\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Build UI inside the persistent container (all callbacks close over THIS instance)\n",
    "# -----------------------------------------------------------------------------\n",
    "with app_out:\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # Local re-entrancy lock (per UI instance)\n",
    "    _refresh_lock = {\"busy\": False}\n",
    "\n",
    "    # Outputs for this instance\n",
    "    out_text = widgets.Output()\n",
    "    out_plot = widgets.Output()\n",
    "\n",
    "    # ------------------------\n",
    "    # Controls (this instance)\n",
    "    # ------------------------\n",
    "    # Data generation controls\n",
    "    n_w = widgets.IntSlider(value=60, min=10, max=300, step=5, description=\"N\")\n",
    "    noise_w = widgets.FloatSlider(value=0.4, min=1e-6, max=1000.0, step=0.1, description=\"noise σ\")\n",
    "    seed_w = widgets.IntSlider(value=42, min=0, max=9999, step=1, description=\"seed\")\n",
    "    a_true_w = widgets.FloatSlider(value=0.6, min=-3.0, max=100.0, step=0.05, description=\"a_true\")\n",
    "    b_true_w = widgets.FloatSlider(value=2.5, min=-10.0, max=10.0, step=0.1, description=\"b_true\")\n",
    "\n",
    "    # Weighting toggle\n",
    "    use_weighting_w = widgets.Checkbox(value=True, description=\"use Wd (target RMS≈1)\")\n",
    "\n",
    "    # Initialization\n",
    "    a0_w = widgets.FloatText(value=0.0, description=\"a0\")\n",
    "    b0_w = widgets.FloatText(value=0.0, description=\"b0\")\n",
    "\n",
    "    # Solver selection (values are exactly what RegressionState.step expects)\n",
    "    optimizer_w = widgets.Dropdown(\n",
    "        options=[(\"Gradient descent\", \"gd\"), (\"Newton\", \"newton\")],\n",
    "        value=\"gd\",\n",
    "        description=\"solver\",\n",
    "    )\n",
    "\n",
    "    # GD control: α in (0,1] (dimensionless)\n",
    "    alpha_gd_w = widgets.FloatSlider(value=0.8, min=0.01, max=1.0, step=0.01, description=\"α (GD)\")\n",
    "\n",
    "    # Newton controls\n",
    "    newton_damp_w = widgets.FloatLogSlider(value=1e-12, base=10, min=-16, max=2, step=0.25, description=\"λ (damp)\")\n",
    "    newton_alpha_w = widgets.FloatSlider(value=1.0, min=0.05, max=1.0, step=0.05, description=\"α (Newton)\")\n",
    "\n",
    "    # Convergence + surface\n",
    "    tol_w = widgets.FloatLogSlider(value=1e-6, base=10, min=-12, max=-2, step=0.1, description=\"tol (||∇||)\")\n",
    "    maxit_w = widgets.IntSlider(value=5000, min=10, max=10000, step=10, description=\"maxit\")\n",
    "    span_w = widgets.FloatSlider(value=2.0, min=0.25, max=6.0, step=0.25, description=\"surface span\")\n",
    "    logsurf_w = widgets.Checkbox(value=False, description=\"log10 surface\")\n",
    "    show_true_w = widgets.Checkbox(value=True, description=\"show true line\")\n",
    "\n",
    "    # Buttons\n",
    "    btn_regen = widgets.Button(description=\"Regenerate data\", button_style=\"primary\")\n",
    "    btn_reset = widgets.Button(description=\"Reset model\", button_style=\"\")\n",
    "    btn_step = widgets.Button(description=\"Step (1 iter)\", button_style=\"success\")\n",
    "    btn_10 = widgets.Button(description=\"Step (10 iters)\", button_style=\"success\")\n",
    "    btn_converge = widgets.Button(description=\"Run to convergence\", button_style=\"warning\")\n",
    "\n",
    "    # State for this instance\n",
    "    state = RegressionState()\n",
    "\n",
    "    # ------------------------\n",
    "    # Helpers (this instance)\n",
    "    # ------------------------\n",
    "    def _solver_label() -> str:\n",
    "        \"\"\"Return a human-readable label for the dropdown solver.\"\"\"\n",
    "        return \"Gradient Descent\" if optimizer_w.value == \"gd\" else \"Newton\"\n",
    "\n",
    "    def _refresh_outputs() -> None:\n",
    "        \"\"\"\n",
    "        Refresh text + plots for this UI instance.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Uses a per-instance lock to prevent duplicate redraws caused by chained widget events.\n",
    "        \"\"\"\n",
    "        if _refresh_lock[\"busy\"]:\n",
    "            return\n",
    "        _refresh_lock[\"busy\"] = True\n",
    "        try:\n",
    "            with out_text:\n",
    "                clear_output(wait=True)\n",
    "                print(state_summary_text(state, _solver_label()))\n",
    "\n",
    "            with out_plot:\n",
    "                clear_output(wait=True)\n",
    "                try:\n",
    "                    plt.close(\"all\")\n",
    "                    plot_fits_and_histories(state, show_true=bool(show_true_w.value))\n",
    "                    plot_misfit_surface(\n",
    "                        state,\n",
    "                        span_factor=float(span_w.value),\n",
    "                        ngrid=140,\n",
    "                        log_surface=bool(logsurf_w.value),\n",
    "                    )\n",
    "                except Exception:\n",
    "                    print(\"Plotting failed. Full traceback:\\n\")\n",
    "                    print(traceback.format_exc())\n",
    "        finally:\n",
    "            _refresh_lock[\"busy\"] = False\n",
    "\n",
    "    def _regenerate_data(_=None) -> None:\n",
    "        \"\"\"Generate a new synthetic dataset and reset the inversion state.\"\"\"\n",
    "        fe, d, d_clean, G, m_true, sigma = make_synthetic_data_matrix(\n",
    "            n=int(n_w.value),\n",
    "            a_true=float(a_true_w.value),\n",
    "            b_true=float(b_true_w.value),\n",
    "            noise_std=float(noise_w.value),\n",
    "            seed=int(seed_w.value),\n",
    "        )\n",
    "\n",
    "        state.fe = fe\n",
    "        state.G = G\n",
    "        state.d = d\n",
    "        state.d_clean = d_clean\n",
    "        state.m_true = m_true\n",
    "        state.sigma = sigma\n",
    "\n",
    "        state.Wd = build_Wd_from_sigma(sigma) if bool(use_weighting_w.value) else None\n",
    "        state.update_normal_equations_solution()\n",
    "        state.reset_model(a0=float(a0_w.value), b0=float(b0_w.value))\n",
    "\n",
    "        _refresh_outputs()\n",
    "\n",
    "    def _reset_model(_=None) -> None:\n",
    "        \"\"\"Reset the current model estimate and histories to the initial guess.\"\"\"\n",
    "        state.reset_model(a0=float(a0_w.value), b0=float(b0_w.value))\n",
    "        _refresh_outputs()\n",
    "\n",
    "    def _step_k(k: int) -> None:\n",
    "        \"\"\"Advance the selected solver by k iterations (bounded by maxit).\"\"\"\n",
    "        method = optimizer_w.value  # \"gd\" or \"newton\"\n",
    "        alpha_gd = float(alpha_gd_w.value)\n",
    "        damp = float(newton_damp_w.value)\n",
    "        alphaN = float(newton_alpha_w.value)\n",
    "        maxit = int(maxit_w.value)\n",
    "\n",
    "        for _ in range(int(k)):\n",
    "            if state.it >= maxit:\n",
    "                break\n",
    "            state.step(method=method, lr=alpha_gd, damping=damp, step_scale=alphaN)\n",
    "\n",
    "    def _on_step(_=None) -> None:\n",
    "        _step_k(1)\n",
    "        _refresh_outputs()\n",
    "\n",
    "    def _on_step10(_=None) -> None:\n",
    "        _step_k(10)\n",
    "        _refresh_outputs()\n",
    "\n",
    "    def _on_converge(_=None) -> None:\n",
    "        \"\"\"\n",
    "        Run until ||∇phi_d|| < tol (GD) or do a single Newton step.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        For this quadratic 2-parameter problem, Newton should jump to the minimizer\n",
    "        in (about) one step (up to damping/step_scale).\n",
    "        \"\"\"\n",
    "        method = optimizer_w.value\n",
    "        alpha_gd = float(alpha_gd_w.value)\n",
    "        damp = float(newton_damp_w.value)\n",
    "        alphaN = float(newton_alpha_w.value)\n",
    "        tol = float(tol_w.value)\n",
    "        maxit = int(maxit_w.value)\n",
    "\n",
    "        if method == \"newton\":\n",
    "            if state.it < maxit:\n",
    "                state.step(method=\"newton\", lr=alpha_gd, damping=damp, step_scale=alphaN)\n",
    "            _refresh_outputs()\n",
    "            return\n",
    "\n",
    "        # Gradient descent: iterate to tolerance\n",
    "        while state.it < maxit:\n",
    "            # ensure history exists\n",
    "            if state.it == 0 and not state.gradnorm_history:\n",
    "                state.step(method=\"gd\", lr=alpha_gd, damping=damp, step_scale=alphaN)\n",
    "\n",
    "            gnorm = state.gradnorm_history[-1] if state.gradnorm_history else np.inf\n",
    "            if gnorm < tol:\n",
    "                break\n",
    "\n",
    "            state.step(method=\"gd\", lr=alpha_gd, damping=damp, step_scale=alphaN)\n",
    "\n",
    "        _refresh_outputs()\n",
    "\n",
    "    # ------------------------\n",
    "    # Wire callbacks (this instance)\n",
    "    # ------------------------\n",
    "    btn_regen.on_click(_regenerate_data)\n",
    "    btn_reset.on_click(_reset_model)\n",
    "    btn_step.on_click(_on_step)\n",
    "    btn_10.on_click(_on_step10)\n",
    "    btn_converge.on_click(_on_converge)\n",
    "\n",
    "    # Make solver changes immediately visible in the printed header (and keep plots consistent)\n",
    "    optimizer_w.observe(lambda ch: _refresh_outputs(), names=\"value\")\n",
    "\n",
    "    def _on_weighting_toggle(_):\n",
    "        \"\"\"Update Wd and recompute m_NE without regenerating data.\"\"\"\n",
    "        if getattr(state, \"sigma\", np.array([])).size == 0:\n",
    "            return\n",
    "        state.Wd = build_Wd_from_sigma(state.sigma) if bool(use_weighting_w.value) else None\n",
    "        state.update_normal_equations_solution()\n",
    "        _refresh_outputs()\n",
    "\n",
    "    use_weighting_w.observe(_on_weighting_toggle, names=\"value\")\n",
    "\n",
    "    # ------------------------\n",
    "    # Layout\n",
    "    # ------------------------\n",
    "    controls_left = widgets.VBox([\n",
    "        widgets.HTML(\"<b>Data generation</b>\"),\n",
    "        n_w, noise_w, seed_w, a_true_w, b_true_w,\n",
    "        use_weighting_w,\n",
    "        btn_regen,\n",
    "    ])\n",
    "\n",
    "    controls_mid = widgets.VBox([\n",
    "        widgets.HTML(\"<b>Initialization</b>\"),\n",
    "        a0_w, b0_w,\n",
    "        btn_reset,\n",
    "        widgets.HTML(\"<b>Optimization</b>\"),\n",
    "        optimizer_w,\n",
    "        alpha_gd_w,\n",
    "        newton_damp_w,\n",
    "        newton_alpha_w,\n",
    "        tol_w, maxit_w,\n",
    "        widgets.HTML(\"<b>Misfit surface</b>\"),\n",
    "        span_w, logsurf_w, show_true_w,\n",
    "    ])\n",
    "\n",
    "    controls_right = widgets.VBox([\n",
    "        widgets.HTML(\"<b>Iterate</b>\"),\n",
    "        btn_step,\n",
    "        btn_10,\n",
    "        btn_converge,\n",
    "    ])\n",
    "\n",
    "    ui = widgets.HBox([controls_left, controls_mid, controls_right])\n",
    "    display(ui, out_text, out_plot)\n",
    "\n",
    "    # Save references so we can close them on next run (including container)\n",
    "    _FE_RHO_APP = {\n",
    "        \"app_out\": app_out,\n",
    "        \"widgets\": [\n",
    "            out_text, out_plot,\n",
    "            n_w, noise_w, seed_w, a_true_w, b_true_w, use_weighting_w,\n",
    "            a0_w, b0_w,\n",
    "            optimizer_w, alpha_gd_w, newton_damp_w, newton_alpha_w,\n",
    "            tol_w, maxit_w, span_w, logsurf_w, show_true_w,\n",
    "            btn_regen, btn_reset, btn_step, btn_10, btn_converge,\n",
    "            ui, controls_left, controls_mid, controls_right\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Initialize once\n",
    "_regenerate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f345996b-0df3-49a4-965c-d1f23ffc7dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b941d37cf98466bbf20d67abc2e056f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'output_type': 'display_data', 'data': {'text/plain': \"HBox(children=(VBox(children=(HTML(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 6) Interactive UI (widgets) - robust, single-instance + per-instance callbacks\n",
    "# =============================================================================\n",
    "\n",
    "import traceback\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Single-instance management\n",
    "# -----------------------------------------------------------------------------\n",
    "# We keep ONE persistent app_out and close prior widget instances on re-run.\n",
    "\n",
    "# Try to reuse an existing app_out if present\n",
    "_prev_app_out = None\n",
    "if \"_FE_RHO_APP\" in globals():\n",
    "    try:\n",
    "        _prev_app_out = _FE_RHO_APP.get(\"app_out\", None)\n",
    "    except Exception:\n",
    "        _prev_app_out = None\n",
    "\n",
    "# Reuse if it's a valid Output widget, else create a new one\n",
    "if isinstance(_prev_app_out, widgets.Output):\n",
    "    app_out = _prev_app_out\n",
    "else:\n",
    "    app_out = widgets.Output()\n",
    "\n",
    "# IMPORTANT: Always display app_out on every run.\n",
    "# Jupyter replaces the cell output on re-run; if you don't re-display, you see nothing.\n",
    "display(app_out)\n",
    "\n",
    "# Close prior widgets (but NEVER close the persistent container app_out)\n",
    "if \"_FE_RHO_APP\" in globals():\n",
    "    try:\n",
    "        for w in _FE_RHO_APP.get(\"widgets\", []):\n",
    "            try:\n",
    "                # Skip closing the persistent container if it ever appears here\n",
    "                if w is app_out:\n",
    "                    continue\n",
    "                w.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Close stray matplotlib figures to avoid blank/extra renders\n",
    "plt.close(\"all\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Build UI inside the persistent container (all callbacks close over THIS instance)\n",
    "# -----------------------------------------------------------------------------\n",
    "with app_out:\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # Local re-entrancy lock (per UI instance)\n",
    "    _refresh_lock = {\"busy\": False}\n",
    "\n",
    "    # Outputs for this instance\n",
    "    out_text = widgets.Output()\n",
    "    out_plot = widgets.Output()\n",
    "\n",
    "    # ------------------------\n",
    "    # Controls (this instance)\n",
    "    # ------------------------\n",
    "    # Data generation controls\n",
    "    n_w = widgets.IntSlider(value=60, min=10, max=300, step=5, description=\"N\")\n",
    "    noise_w = widgets.FloatSlider(value=0.4, min=1e-6, max=1000.0, step=0.1, description=\"noise σ\")\n",
    "    seed_w = widgets.IntSlider(value=42, min=0, max=9999, step=1, description=\"seed\")\n",
    "    a_true_w = widgets.FloatSlider(value=0.6, min=-3.0, max=100.0, step=0.05, description=\"a_true\")\n",
    "    b_true_w = widgets.FloatSlider(value=2.5, min=-10.0, max=10.0, step=0.1, description=\"b_true\")\n",
    "\n",
    "    # Weighting toggle\n",
    "    use_weighting_w = widgets.Checkbox(value=True, description=\"use Wd (target RMS≈1)\")\n",
    "\n",
    "    # Initialization\n",
    "    a0_w = widgets.FloatText(value=0.0, description=\"a0\")\n",
    "    b0_w = widgets.FloatText(value=0.0, description=\"b0\")\n",
    "\n",
    "    # Solver selection (values are exactly what RegressionState.step expects)\n",
    "    optimizer_w = widgets.Dropdown(\n",
    "        options=[(\"Gradient descent\", \"gd\"), (\"Newton\", \"newton\")],\n",
    "        value=\"gd\",\n",
    "        description=\"solver\",\n",
    "    )\n",
    "\n",
    "    # GD control: α in (0,1] (dimensionless)\n",
    "    alpha_gd_w = widgets.FloatSlider(value=0.8, min=0.01, max=1.0, step=0.01, description=\"α (GD)\")\n",
    "\n",
    "    # Newton controls\n",
    "    newton_damp_w = widgets.FloatLogSlider(value=1e-12, base=10, min=-16, max=2, step=0.25, description=\"λ (damp)\")\n",
    "    newton_alpha_w = widgets.FloatSlider(value=1.0, min=0.05, max=1.0, step=0.05, description=\"α (Newton)\")\n",
    "\n",
    "    # Convergence + surface\n",
    "    tol_w = widgets.FloatLogSlider(value=1e-6, base=10, min=-12, max=-2, step=0.1, description=\"tol (||∇||)\")\n",
    "    maxit_w = widgets.IntSlider(value=5000, min=10, max=10000, step=10, description=\"maxit\")\n",
    "    span_w = widgets.FloatSlider(value=2.0, min=0.25, max=6.0, step=0.25, description=\"surface span\")\n",
    "    logsurf_w = widgets.Checkbox(value=False, description=\"log10 surface\")\n",
    "    show_true_w = widgets.Checkbox(value=True, description=\"show true line\")\n",
    "\n",
    "    # Buttons\n",
    "    btn_regen = widgets.Button(description=\"Regenerate data\", button_style=\"primary\")\n",
    "    btn_reset = widgets.Button(description=\"Reset model\", button_style=\"\")\n",
    "    btn_step = widgets.Button(description=\"Step (1 iter)\", button_style=\"success\")\n",
    "    btn_10 = widgets.Button(description=\"Step (10 iters)\", button_style=\"success\")\n",
    "    btn_converge = widgets.Button(description=\"Run to convergence\", button_style=\"warning\")\n",
    "\n",
    "    # State for this instance\n",
    "    state = RegressionState()\n",
    "\n",
    "    # ------------------------\n",
    "    # Helpers (this instance)\n",
    "    # ------------------------\n",
    "    def _solver_label() -> str:\n",
    "        \"\"\"Return a human-readable label for the dropdown solver.\"\"\"\n",
    "        return \"Gradient Descent\" if optimizer_w.value == \"gd\" else \"Newton\"\n",
    "\n",
    "    def _refresh_outputs() -> None:\n",
    "        \"\"\"\n",
    "        Refresh text + plots for this UI instance.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Uses a per-instance lock to prevent duplicate redraws caused by chained widget events.\n",
    "        \"\"\"\n",
    "        if _refresh_lock[\"busy\"]:\n",
    "            return\n",
    "        _refresh_lock[\"busy\"] = True\n",
    "        try:\n",
    "            with out_text:\n",
    "                clear_output(wait=True)\n",
    "                print(state_summary_text(state, _solver_label()))\n",
    "\n",
    "            with out_plot:\n",
    "                clear_output(wait=True)\n",
    "                try:\n",
    "                    plt.close(\"all\")\n",
    "                    plot_fits_and_histories(state, show_true=bool(show_true_w.value))\n",
    "                    plot_misfit_surface(\n",
    "                        state,\n",
    "                        span_factor=float(span_w.value),\n",
    "                        ngrid=140,\n",
    "                        log_surface=bool(logsurf_w.value),\n",
    "                    )\n",
    "                except Exception:\n",
    "                    print(\"Plotting failed. Full traceback:\\n\")\n",
    "                    print(traceback.format_exc())\n",
    "        finally:\n",
    "            _refresh_lock[\"busy\"] = False\n",
    "\n",
    "    def _regenerate_data(_=None) -> None:\n",
    "        \"\"\"Generate a new synthetic dataset and reset the inversion state.\"\"\"\n",
    "        fe, d, d_clean, G, m_true, sigma = make_synthetic_data_matrix(\n",
    "            n=int(n_w.value),\n",
    "            a_true=float(a_true_w.value),\n",
    "            b_true=float(b_true_w.value),\n",
    "            noise_std=float(noise_w.value),\n",
    "            seed=int(seed_w.value),\n",
    "        )\n",
    "\n",
    "        state.fe = fe\n",
    "        state.G = G\n",
    "        state.d = d\n",
    "        state.d_clean = d_clean\n",
    "        state.m_true = m_true\n",
    "        state.sigma = sigma\n",
    "\n",
    "        state.Wd = build_Wd_from_sigma(sigma) if bool(use_weighting_w.value) else None\n",
    "        state.update_normal_equations_solution()\n",
    "        state.reset_model(a0=float(a0_w.value), b0=float(b0_w.value))\n",
    "\n",
    "        _refresh_outputs()\n",
    "\n",
    "    def _reset_model(_=None) -> None:\n",
    "        \"\"\"Reset the current model estimate and histories to the initial guess.\"\"\"\n",
    "        state.reset_model(a0=float(a0_w.value), b0=float(b0_w.value))\n",
    "        _refresh_outputs()\n",
    "\n",
    "    def _step_k(k: int) -> None:\n",
    "        \"\"\"Advance the selected solver by k iterations (bounded by maxit).\"\"\"\n",
    "        method = optimizer_w.value  # \"gd\" or \"newton\"\n",
    "        alpha_gd = float(alpha_gd_w.value)\n",
    "        damp = float(newton_damp_w.value)\n",
    "        alphaN = float(newton_alpha_w.value)\n",
    "        maxit = int(maxit_w.value)\n",
    "\n",
    "        for _ in range(int(k)):\n",
    "            if state.it >= maxit:\n",
    "                break\n",
    "            state.step(method=method, lr=alpha_gd, damping=damp, step_scale=alphaN)\n",
    "\n",
    "    def _on_step(_=None) -> None:\n",
    "        _step_k(1)\n",
    "        _refresh_outputs()\n",
    "\n",
    "    def _on_step10(_=None) -> None:\n",
    "        _step_k(10)\n",
    "        _refresh_outputs()\n",
    "\n",
    "    def _on_converge(_=None) -> None:\n",
    "        \"\"\"\n",
    "        Run until ||∇phi_d|| < tol (GD) or do a single Newton step.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        For this quadratic 2-parameter problem, Newton should jump to the minimizer\n",
    "        in (about) one step (up to damping/step_scale).\n",
    "        \"\"\"\n",
    "        method = optimizer_w.value\n",
    "        alpha_gd = float(alpha_gd_w.value)\n",
    "        damp = float(newton_damp_w.value)\n",
    "        alphaN = float(newton_alpha_w.value)\n",
    "        tol = float(tol_w.value)\n",
    "        maxit = int(maxit_w.value)\n",
    "\n",
    "        if method == \"newton\":\n",
    "            if state.it < maxit:\n",
    "                state.step(method=\"newton\", lr=alpha_gd, damping=damp, step_scale=alphaN)\n",
    "            _refresh_outputs()\n",
    "            return\n",
    "\n",
    "        # Gradient descent: iterate to tolerance\n",
    "        while state.it < maxit:\n",
    "            # ensure history exists\n",
    "            if state.it == 0 and not state.gradnorm_history:\n",
    "                state.step(method=\"gd\", lr=alpha_gd, damping=damp, step_scale=alphaN)\n",
    "\n",
    "            gnorm = state.gradnorm_history[-1] if state.gradnorm_history else np.inf\n",
    "            if gnorm < tol:\n",
    "                break\n",
    "\n",
    "            state.step(method=\"gd\", lr=alpha_gd, damping=damp, step_scale=alphaN)\n",
    "\n",
    "        _refresh_outputs()\n",
    "\n",
    "    # ------------------------\n",
    "    # Wire callbacks (this instance)\n",
    "    # ------------------------\n",
    "    btn_regen.on_click(_regenerate_data)\n",
    "    btn_reset.on_click(_reset_model)\n",
    "    btn_step.on_click(_on_step)\n",
    "    btn_10.on_click(_on_step10)\n",
    "    btn_converge.on_click(_on_converge)\n",
    "\n",
    "    # Make solver changes immediately visible in the printed header (and keep plots consistent)\n",
    "    optimizer_w.observe(lambda ch: _refresh_outputs(), names=\"value\")\n",
    "\n",
    "    def _on_weighting_toggle(_):\n",
    "        \"\"\"Update Wd and recompute m_NE without regenerating data.\"\"\"\n",
    "        if getattr(state, \"sigma\", np.array([])).size == 0:\n",
    "            return\n",
    "        state.Wd = build_Wd_from_sigma(state.sigma) if bool(use_weighting_w.value) else None\n",
    "        state.update_normal_equations_solution()\n",
    "        _refresh_outputs()\n",
    "\n",
    "    use_weighting_w.observe(_on_weighting_toggle, names=\"value\")\n",
    "\n",
    "    # ------------------------\n",
    "    # Layout\n",
    "    # ------------------------\n",
    "    controls_left = widgets.VBox([\n",
    "        widgets.HTML(\"<b>Data generation</b>\"),\n",
    "        n_w, noise_w, seed_w, a_true_w, b_true_w,\n",
    "        use_weighting_w,\n",
    "        btn_regen,\n",
    "    ])\n",
    "\n",
    "    controls_mid = widgets.VBox([\n",
    "        widgets.HTML(\"<b>Initialization</b>\"),\n",
    "        a0_w, b0_w,\n",
    "        btn_reset,\n",
    "        widgets.HTML(\"<b>Optimization</b>\"),\n",
    "        optimizer_w,\n",
    "        alpha_gd_w,\n",
    "        newton_damp_w,\n",
    "        newton_alpha_w,\n",
    "        tol_w, maxit_w,\n",
    "        widgets.HTML(\"<b>Misfit surface</b>\"),\n",
    "        span_w, logsurf_w, show_true_w,\n",
    "    ])\n",
    "\n",
    "    controls_right = widgets.VBox([\n",
    "        widgets.HTML(\"<b>Iterate</b>\"),\n",
    "        btn_step,\n",
    "        btn_10,\n",
    "        btn_converge,\n",
    "    ])\n",
    "\n",
    "    ui = widgets.HBox([controls_left, controls_mid, controls_right])\n",
    "    display(ui, out_text, out_plot)\n",
    "\n",
    "    # Save references so we can close them on next run (including container)\n",
    "    _FE_RHO_APP = {\n",
    "        \"app_out\": app_out,\n",
    "        \"widgets\": [\n",
    "            # DO NOT include app_out here; we want to reuse it forever\n",
    "            out_text, out_plot,\n",
    "            n_w, noise_w, seed_w, a_true_w, b_true_w, use_weighting_w,\n",
    "            a0_w, b0_w,\n",
    "            optimizer_w, alpha_gd_w, newton_damp_w, newton_alpha_w,\n",
    "            tol_w, maxit_w, span_w, logsurf_w, show_true_w,\n",
    "            btn_regen, btn_reset, btn_step, btn_10, btn_converge,\n",
    "            ui, controls_left, controls_mid, controls_right\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Initialize once (inside the container, so the output updates immediately)\n",
    "    _regenerate_data()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
